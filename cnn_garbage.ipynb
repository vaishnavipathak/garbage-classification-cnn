{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcaAltreuUZpaoJnCOUX9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavipathak/garbage-classification-cnn/blob/main/cnn_garbage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import wandb\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "!pip install wandb gdown\n",
        "!wandb login\n",
        "\n",
        "!gdown '1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6'\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content/garbage_data/\"\n",
        "!unzip -q Garbage_classification.zip -d {DATA_DIR}\n",
        "\n",
        "print(\"Setup complete. Dataset downloaded and unzipped.\")"
      ],
      "metadata": {
        "id": "6iP7OS9ldlsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86a8e23-d580-493e-df21-ede69af5b819"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvaishnavipathak\u001b[0m (\u001b[33mvaishnavipathak-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6\n",
            "From (redirected): https://drive.google.com/uc?id=1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6&confirm=t&uuid=4f5bbec0-cd93-4a32-abef-d628b0e6be98\n",
            "To: /content/Garbage_classification.zip\n",
            "100% 43.0M/43.0M [00:01<00:00, 32.0MB/s]\n",
            "Setup complete. Dataset downloaded and unzipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformations\n",
        "IMG_SIZE = 224\n",
        "# We create two pipelines.\n",
        "# The train transform randomly flips and rotates images for data augmentation.\n",
        "# The val transform does not, ensuring we validate on unaltered images.\n",
        "# Both pipelines resize, convert images to PyTorch Tensors, and normalize them.\n",
        "#  we took Image size as 224*224 and batch size as 32.\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load and Split the Dataset\n",
        "dataset_folder = os.path.join(DATA_DIR, os.listdir(DATA_DIR)[0])\n",
        "full_dataset = datasets.ImageFolder(dataset_folder)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset)) # 80% training data and 20% data for vatidation.\n",
        "val_size = len(full_dataset) - train_size\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "train_dataset.dataset.transform = data_transforms['train']\n",
        "val_dataset.dataset.transform = data_transforms['val']\n",
        "\n",
        "#Create DataLoaders\n",
        "#  Data Loaders is powerful PyTorch tool that groups our dataset into batches and shuffles the training data each epoch to improve learning.\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Verify\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "print(f\"Training images: {len(train_dataset)}\")\n",
        "print(f\"Validation images: {len(val_dataset)}\")\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch of images shape: {images.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH3JR4H0aGJF",
        "outputId": "3df15569-1a62-4633-dc50-1a522404bac7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 2527\n",
            "Training images: 2021\n",
            "Validation images: 506\n",
            "\n",
            "Batch of images shape: torch.Size([32, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class SimpleCNN defines our model as a Python class that inherits from PyTorch's base neural network module.\n",
        "\n",
        "init is the constructor where we define all the layers our network will use.\n",
        "\n",
        "I have grouped them into two nn.Sequential blocks: self.features for the convolutional part and self.classifier for the final dense layers.\n"
      ],
      "metadata": {
        "id": "fDEZy46vkGW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=6, dense_neurons=256, dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        Initializes the SimpleCNN model.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): The number of output classes (6 for our dataset).\n",
        "            dense_neurons (int): The number of neurons in the hidden dense layer.\n",
        "            dropout_rate (float): The dropout probability.\n",
        "        \"\"\"\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # 5 Conv-ReLU-MaxPool blocks as the \"feature extractor\".\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # defines the dense layer and output layer as the \"classifier\"\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=32 * 7 * 7, out_features=dense_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(in_features=dense_neurons, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines how data flows through the network.\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9cg0Jt6i0hg",
        "outputId": "c9fbc1a7-9f69-4a98-e9f8-cd9aaecc5902"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU()\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=1568, out_features=256, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=256, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# instance of the model class we defined earlier\n",
        "model = SimpleCNN()\n",
        "# following command moves the model's parameters and computations onto the GPU.\n",
        "model.to(device)\n",
        "\n",
        "# Instantiate Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Model, criterion, and optimizer are ready and configured for GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dbRLCipmfUq",
        "outputId": "24dffae4-8c11-4c26-c01b-6c321b058f6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model, criterion, and optimizer are ready and configured for GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "\n",
        "    # Loop over the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over the training data\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move inputs and labels to the GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass, compute predicted outputs\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass, compute gradient of the loss\n",
        "            loss.backward()\n",
        "\n",
        "            #Step: update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate running statistics\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        # Disable gradient calculation for validation to save memory and computations.\n",
        "        with torch.no_grad():\n",
        "            # Iterate over the validation data\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\\n\")\n",
        "\n",
        "    print(\"Training Complete\")\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUnqMR4fpaKK",
        "outputId": "08443041-dd3f-4931-adbd-cfe0f9badd04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Epoch 1/10\n",
            "Train Loss: 1.2746, Train Acc: 0.4859 | Val Loss: 1.2530, Val Acc: 0.5257\n",
            "\n",
            "Starting Epoch 2/10\n",
            "Train Loss: 1.1464, Train Acc: 0.5552 | Val Loss: 1.2408, Val Acc: 0.5138\n",
            "\n",
            "Starting Epoch 3/10\n",
            "Train Loss: 1.0817, Train Acc: 0.5898 | Val Loss: 1.0559, Val Acc: 0.6067\n",
            "\n",
            "Starting Epoch 4/10\n",
            "Train Loss: 0.9948, Train Acc: 0.6195 | Val Loss: 1.0058, Val Acc: 0.6462\n",
            "\n",
            "Starting Epoch 5/10\n",
            "Train Loss: 0.9174, Train Acc: 0.6551 | Val Loss: 1.1207, Val Acc: 0.5968\n",
            "\n",
            "Starting Epoch 6/10\n",
            "Train Loss: 0.8556, Train Acc: 0.6843 | Val Loss: 0.9959, Val Acc: 0.6344\n",
            "\n",
            "Starting Epoch 7/10\n",
            "Train Loss: 0.8305, Train Acc: 0.6883 | Val Loss: 0.8835, Val Acc: 0.6976\n",
            "\n",
            "Starting Epoch 8/10\n",
            "Train Loss: 0.7431, Train Acc: 0.7279 | Val Loss: 0.9158, Val Acc: 0.6719\n",
            "\n",
            "Starting Epoch 9/10\n",
            "Train Loss: 0.6968, Train Acc: 0.7288 | Val Loss: 0.8316, Val Acc: 0.7115\n",
            "\n",
            "Starting Epoch 10/10\n",
            "Train Loss: 0.6377, Train Acc: 0.7630 | Val Loss: 0.8378, Val Acc: 0.7115\n",
            "\n",
            "Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U13AGE_7kE_K"
      }
    }
  ]
}