{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPF4N0pHOnDCOlt2YKzXRw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavipathak/garbage-classification-cnn/blob/main/cnn_assignment_partB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neATy0DlQM22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c12fce-a443-45ae-a49e-c682e5bc7765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6\n",
            "From (redirected): https://drive.google.com/uc?id=1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6&confirm=t&uuid=a2ee1a03-9ea9-44b6-8559-0378a4597aa7\n",
            "To: /content/Garbage_classification.zip\n",
            "100% 43.0M/43.0M [00:01<00:00, 32.9MB/s]\n",
            "got the Dataset\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# from torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "import os\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "!gdown '1cqAJ3b_TIEqHnKQGLGjxCBKS3MUatGa6'\n",
        "DATA_DIR = \"/content/garbage_data/\"\n",
        "!unzip -q Garbage_classification.zip -d {DATA_DIR}\n",
        "\n",
        "print(\"got the Dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "dataset_folder = os.path.join(DATA_DIR, os.listdir(DATA_DIR)[0])\n",
        "full_dataset = datasets.ImageFolder(dataset_folder)\n",
        "\n",
        "val_split = 0.20\n",
        "test_split = 0.20\n",
        "train_size = int((1.0 - val_split - test_split) * len(full_dataset))\n",
        "val_size = int(val_split * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "print(f\"\\nSplitting {len(full_dataset)} images into:\")\n",
        "print(f\"- Training: {train_size} (60%)\")\n",
        "print(f\"- Validation: {val_size} (20%)\")\n",
        "print(f\"- Test: {test_size} (20%)\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "# class to apply transformations to subset\n",
        "class DatasetWithTransforms(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.subset[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "train_subset, val_subset, test_subset = random_split(full_dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "train_dataset = DatasetWithTransforms(train_subset, transform=data_transforms['train'])\n",
        "val_dataset = DatasetWithTransforms(val_subset, transform=data_transforms['val'])\n",
        "test_dataset = DatasetWithTransforms(test_subset, transform=data_transforms['val'])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"\\nDataLoaders with the correct 60/20/20 split are now ready.\")\n",
        "print(\"Train DataLoader length:\", len(train_loader.dataset))\n",
        "print(\"Val DataLoader length:\", len(val_loader.dataset))\n",
        "print(\"Test DataLoader length:\", len(test_loader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXiivaj5PYss",
        "outputId": "eff48fc0-708b-49dd-c811-e413934f3274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Splitting 2527 images into:\n",
            "- Training: 1516 (60%)\n",
            "- Validation: 505 (20%)\n",
            "- Test: 506 (20%)\n",
            "\n",
            "DataLoaders with the correct 60/20/20 split are now ready.\n",
            "Train DataLoader length: 1516\n",
            "Val DataLoader length: 505\n",
            "Test DataLoader length: 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights='IMAGENET1K_V2' loads the model with its learned weights\n",
        "model = models.resnet50(weights='IMAGENET1K_V2')\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "model.fc = nn.Linear(num_ftrs, 6)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model loaded, frozen, and modified for fine tuning\")\n",
        "print(\"New final layer:\", model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-nOXadvRYgf",
        "outputId": "973a6d67-8742-43f5-f267-f89963507ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 104MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded, frozen, and modified for fine-tuning.\n",
            "New final layer: Linear(in_features=2048, out_features=6, bias=True)\n"
          ]
        }
      ]
    }
  ]
}